#From the given ‘Iris’ dataset, predict the optimum number of clusters and represent it visually.

#Prediction using unsupervised ML.

#Load the required libraries

library(ggplot2)

#Reading the data using read.csv() function and assign it to the variable "i"# 

i<-read.csv( "C:\\Users\\hp\\Downloads\\Iris - Iris.csv" )
i_df<-data.frame(i)
DT::datatable(i_df)
summary(i_df)

#EDA of the dataset 

sapply(i_df[,-5],var)
ggplot(i_df,aes(x=SepalLengthCm,y=SepalWidthCm,col=Species))+geom_point()
ggplot(i_df,aes(x=PetalLengthCm,y=PetalWidthCm,col=Species))+geom_point()


#Clustering algorithm
#We will now get a range of values from 1 to 10 and will try to find the optimum number of clusters using kmeans clustering algorithm#

set.seed(200)
k.max<-10
wss<-sapply(1:k.max,function(k){kmeans(i_df[,3:4],k,nstart = 20,iter.max=20)$tot.withinss})
wss

#Plotting the values with values of k to get a visualisation

plot(1:k.max,wss,type="b",xlab="Number of clusters", ylab="Within cluster sum of squares")
#From the above plot we saw that the optimum value of k is 3. As after k=3 the change in values is constant. Thus we take k=3 for our calculations.

#Applying value of k as 3

icluster<- kmeans(i_df[,3:4],3,nstart = 20)
table(icluster$cluster,i_df$Species)

#Plotting the clusters

plot(i_df[c("PetalLengthCm","PetalWidthCm")],col=icluster$cluster)
plot(i_df[c("SepalLengthCm","SepalWidthCm")],col=icluster$cluster)

#Inference: For the iris dataset the optimal value of k is 3. 